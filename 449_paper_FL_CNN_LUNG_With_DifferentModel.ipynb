{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NFfnoiheFdz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nV0FrrHeXiy",
        "outputId": "6ad133b1-43ee-4876-d532-3254823749d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/federated.zip"
      ],
      "metadata": {
        "id": "XSFTLa4Weg3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import gc\n",
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "\n",
        "\n",
        "#from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "#policy = mixed_precision.Policy('mixed_float16')\n",
        "#mixed_precision.set_policy(policy)\n",
        "#print('Compute dtype: %s' % policy.compute_dtype)\n",
        "#print('Variable dtype: %s' % policy.variable_dtype)"
      ],
      "metadata": {
        "id": "v--pZXNVeQnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATADIR = \"/content/trainingSet\"\n",
        "\n",
        "CATEGORIES = [\"lung_aca\", \"lung_n\", \"lung_scc\"]\n",
        "\n",
        "training_data = []\n",
        "\n",
        "IMG_SIZE = 75\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  # do dogs and cats\n",
        "\n",
        "        path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                \n",
        "                training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                print(e)\n",
        "            #except OSError as e:\n",
        "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "            #except Exception as e:\n",
        "            #    print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "create_training_data()\n",
        "\n",
        "print(len(training_data))\n"
      ],
      "metadata": {
        "id": "x7t4V-JxfgW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "116878db-f594-44c0-df9c-7a22b1798324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2002/2002 [00:13<00:00, 146.81it/s]\n",
            "100%|██████████| 2002/2002 [00:13<00:00, 149.23it/s]\n",
            "100%|██████████| 2002/2002 [00:15<00:00, 132.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/testSet_WithCatagory.zip"
      ],
      "metadata": {
        "id": "JLYotAdnowT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATADIR = \"/content/testSet_WithCatagory\"\n",
        "\n",
        "CATEGORIES = [\"lung_aca\", \"lung_n\", \"lung_scc\"]\n",
        "\n",
        "testing_data = []\n",
        "\n",
        "IMG_SIZE = 75\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  # do dogs and cats\n",
        "\n",
        "        path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
        "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
        "\n",
        "        for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img))  # convert to array\n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "                \n",
        "                testing_data.append([new_array, class_num])  # add this to our training_data\n",
        "            except Exception as e:  # in the interest in keeping the output clean...\n",
        "                print(e)\n",
        "            #except OSError as e:\n",
        "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
        "            #except Exception as e:\n",
        "            #    print(\"general exception\", e, os.path.join(path,img))\n",
        "\n",
        "create_training_data()"
      ],
      "metadata": {
        "id": "nBHtsUcXeoO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19cfb183-4976-4a5f-ee9b-ad8a836379c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 192/192 [00:01<00:00, 143.71it/s]\n",
            "100%|██████████| 208/208 [00:01<00:00, 150.52it/s]\n",
            "100%|██████████| 192/192 [00:01<00:00, 146.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(training_data)\n",
        "random.shuffle(testing_data)"
      ],
      "metadata": {
        "id": "8c6_3fG1pL83"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    X.append(features)\n",
        "    y.append(label)\n",
        "    \n",
        "\n",
        "X_train = np.array(X)\n",
        "X_train = X_train\n",
        "y_train = np.array(y)\n",
        "\n",
        "\n",
        "X_t = []\n",
        "y_t = []\n",
        "\n",
        "for features,label in testing_data:\n",
        "    X_t.append(features)\n",
        "    y_t.append(label)\n",
        "\n",
        "X_test = np.array(X_t)\n",
        "X_test = X_test\n",
        "y_test = np.array(y_t)"
      ],
      "metadata": {
        "id": "jSVn4utvpUyT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "2_lrATp3pVv-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "id": "XbnzA4Z_pXPV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af860f95-ad3b-4bc0-9230-a06b5ae44961"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "592"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "metadata": {
        "id": "luagoSDppYjR"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
        "    ''' return: a dictionary with keys clients' names and value as \n",
        "                data shards - tuple of images and label lists.\n",
        "        args: \n",
        "            image_list: a list of numpy arrays of training images\n",
        "            label_list:a list of binarized labels for each image\n",
        "            num_client: number of fedrated members (clients)\n",
        "            initials: the clients'name prefix, e.g, clients_1 \n",
        "            \n",
        "    '''\n",
        "\n",
        "    #create a list of client names\n",
        "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
        "    \n",
        "    data = list(zip(image_list, label_list))\n",
        "\n",
        "    #shard data and place at each client\n",
        "    size = len(data)//num_clients\n",
        "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
        "\n",
        "    #number of clients must equal number of shards\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
      ],
      "metadata": {
        "id": "KWW8fU3tpacd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients = create_clients(X_train, y_train, num_clients=10, initial='client')"
      ],
      "metadata": {
        "id": "zNRP_f2Upcla"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_data(data_shard, bs=64):\n",
        "    '''Takes in a clients data shard and create a tfds object off it\n",
        "    args:\n",
        "        shard: a data, label constituting a client's data shard\n",
        "        bs:batch size\n",
        "    return:\n",
        "        tfds object'''\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(bs)"
      ],
      "metadata": {
        "id": "gcotdlHxpfg_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process and batch the training data for each client\n",
        "clients_batched = dict()\n",
        "co = 0\n",
        "for (client_name, data) in clients.items():\n",
        "    co+=1\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "    \n",
        "   \n",
        "    \n",
        "#process and batch the test set  \n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
      ],
      "metadata": {
        "id": "pErHzxplphpH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batched"
      ],
      "metadata": {
        "id": "r-eRIetSXJEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1fe4f28-38b6-4768-8e0b-6258d38ffbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clients_batched"
      ],
      "metadata": {
        "id": "qHnsADu4pj1s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81473221-9dc8-4323-9ec5-c496a7bb04e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'client_1': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_2': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_3': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_4': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_5': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_6': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_7': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_8': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_9': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>,\n",
              " 'client_10': <BatchDataset element_spec=(TensorSpec(shape=(None, 75, 75, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 3), dtype=tf.float32, name=None))>}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del(X_train)\n",
        "del(y_train)\n",
        "del(training_data)"
      ],
      "metadata": {
        "id": "Dhu34vkaptef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "class SimpleModel:\n",
        "    def build(self):\n",
        "        model = VGG19(include_top=False,input_shape=(75, 75, 3), weights='imagenet')\n",
        "        transfer_layer = model.get_layer('block5_pool')\n",
        "        conv_model = Model(inputs=model.input, outputs=transfer_layer.output)\n",
        "        # Start a new Keras Sequential model.\n",
        "        new_model = Sequential()\n",
        "\n",
        "        # Add the convolutional part of the VGG16 model from above.\n",
        "        new_model.add(conv_model)\n",
        "\n",
        "        # Flatten the output of the VGG16 model because it is from a\n",
        "        # convolutional layer.\n",
        "        new_model.add(Flatten())\n",
        "\n",
        "        # Add a dense (aka. fully-connected) layer.\n",
        "        # This is for combining features that the VGG16 model has\n",
        "        # recognized in the image.\n",
        "\n",
        "        new_model.add(Dense(512, activation='relu'))\n",
        "\n",
        "\n",
        "        # Add the final layer for the actual classification.\n",
        "        new_model.add(Dense(3, activation='softmax'))\n",
        "        \n",
        "        model1 = InceptionV3(include_top=False,input_shape=(75, 75, 3), weights='imagenet')\n",
        "        transfer_layer1 = model1.get_layer('mixed10')\n",
        "        conv_model1 = Model(inputs=model1.input, outputs=transfer_layer1.output)\n",
        "        new_model1 = Sequential()\n",
        "\n",
        "        new_model1.add(conv_model1)\n",
        "\n",
        "        new_model1.add(Flatten())\n",
        "\n",
        "        #new_model1.add(Dropout(0.5))\n",
        "\n",
        "        new_model1.add(Dense(512, activation='relu'))\n",
        "\n",
        "        # Add the final layer for the actual classification.\n",
        "        new_model1.add(Dense(3, activation='softmax'))\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        model1 = DenseNet121(include_top=False,input_shape=(75, 75, 3), weights='imagenet')\n",
        "        transfer_layer1 = model1.get_layer('bn')\n",
        "        conv_model2 = Model(inputs=model1.input, outputs=transfer_layer1.output)\n",
        "        new_model2 = Sequential()\n",
        "\n",
        "        new_model2.add(conv_model1)\n",
        "\n",
        "        new_model2.add(Flatten())\n",
        "\n",
        "        #new_model1.add(Dropout(0.5))\n",
        "\n",
        "        new_model2.add(Dense(512, activation='relu'))\n",
        "\n",
        "        # Add the final layer for the actual classification.\n",
        "        new_model2.add(Dense(3, activation='softmax'))\n",
        "        \n",
        "        inputs = keras.Input(shape=(75,75,3))\n",
        "        \n",
        "        outputs = layers.average([new_model(inputs),new_model1(inputs),new_model2(inputs)])\n",
        "        ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        \n",
        "        \n",
        "        return ensemble_model\n",
        "        "
      ],
      "metadata": {
        "id": "A4hMJJoW-WK0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = Adam(lr=.00001)\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['categorical_accuracy']"
      ],
      "metadata": {
        "id": "GAD2lQAdp1Yu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name):\n",
        "    client_names = list(clients_trn_data.keys())\n",
        "    #get the bs\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    #first calculate the total training data points across clinets\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
        "    print(global_count)\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
        "    return local_count/global_count\n",
        "\n",
        "\n",
        "def scale_model_weights(weight, scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "    return weight_final\n",
        "\n",
        "\n",
        "\n",
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "        \n",
        "    return avg_grad\n",
        "\n",
        "\n",
        "def test_model(X_test, Y_test,  model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    #logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
        "    return acc, loss"
      ],
      "metadata": {
        "id": "dMxwqAUYp33M"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smlp_global = SimpleModel()\n",
        "global_model = smlp_global.build()\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "local_model = smlp_global.build()\n",
        "local_model.compile(optimizer= 'adam', loss=loss, metrics=metrics)"
      ],
      "metadata": {
        "id": "KoLopF76p6fY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e311afa5-e918-4342-9b70-75774d7e3f66"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 3s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "comms_round = 20\n",
        "for comm_round in range(comms_round):\n",
        "            \n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    \n",
        "    \n",
        "    global_weights = global_model.get_weights()\n",
        "    \n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    \n",
        "    #random.shuffle(client_names)\n",
        "    \n",
        "    #loop through each client and create new local model\n",
        "    count = 0\n",
        "    for client in client_names:\n",
        "        \n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "        \n",
        "        #fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=1)\n",
        "        test_model(X_test, y_test, local_model, comm_round)\n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "        \n",
        "        #clear session to free memory after each communication round\n",
        "        \n",
        "        #K.clear_session()\n",
        "        #gc.collect()\n",
        "        #del local_model\n",
        "    \n",
        "        \n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "    \n",
        "    #update global model \n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
      ],
      "metadata": {
        "id": "HsXSPtRap9RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f04d14-8a3e-436d-c8fb-4a9c334e5ede"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 30s 573ms/step - loss: 1.0905 - categorical_accuracy: 0.4633\n",
            "19/19 [==============================] - 6s 114ms/step\n",
            "comm_round: 0 | global_acc: 32.432% | global_loss: 1.0986121892929077\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.9061 - categorical_accuracy: 0.5517\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 0 | global_acc: 33.953% | global_loss: 1.1179436445236206\n",
            "6400\n",
            "10/10 [==============================] - 3s 255ms/step - loss: 0.8436 - categorical_accuracy: 0.6067\n",
            "19/19 [==============================] - 1s 53ms/step\n",
            "comm_round: 0 | global_acc: 35.135% | global_loss: 1.1255894899368286\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.8327 - categorical_accuracy: 0.6067\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 0 | global_acc: 33.108% | global_loss: 1.098611831665039\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.8048 - categorical_accuracy: 0.6550\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 0 | global_acc: 35.135% | global_loss: 1.1263028383255005\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 1.0166 - categorical_accuracy: 0.4600\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 0 | global_acc: 35.135% | global_loss: 1.1991537809371948\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.7959 - categorical_accuracy: 0.6483\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 0 | global_acc: 35.135% | global_loss: 1.1263020038604736\n",
            "6400\n",
            "10/10 [==============================] - 3s 249ms/step - loss: 0.8421 - categorical_accuracy: 0.6050\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 0 | global_acc: 35.135% | global_loss: 1.126302719116211\n",
            "6400\n",
            "10/10 [==============================] - 3s 253ms/step - loss: 0.8970 - categorical_accuracy: 0.5667\n",
            "19/19 [==============================] - 1s 52ms/step\n",
            "comm_round: 0 | global_acc: 30.912% | global_loss: 1.0974534749984741\n",
            "6400\n",
            "10/10 [==============================] - 3s 251ms/step - loss: 0.7505 - categorical_accuracy: 0.7083\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 0 | global_acc: 33.446% | global_loss: 1.1097770929336548\n",
            "6400\n",
            "19/19 [==============================] - 3s 52ms/step\n",
            "comm_round: 0 | global_acc: 32.939% | global_loss: 1.1660351753234863\n",
            "10/10 [==============================] - 2s 244ms/step - loss: 0.5209 - categorical_accuracy: 0.8867\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 1 | global_acc: 34.797% | global_loss: 1.1119273900985718\n",
            "6400\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.4363 - categorical_accuracy: 0.9250\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 1 | global_acc: 35.135% | global_loss: 1.1262568235397339\n",
            "6400\n",
            "10/10 [==============================] - 2s 246ms/step - loss: 0.4708 - categorical_accuracy: 0.9067\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 1 | global_acc: 32.601% | global_loss: 1.1329739093780518\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.4781 - categorical_accuracy: 0.9067\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 1 | global_acc: 31.419% | global_loss: 1.1329399347305298\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.4369 - categorical_accuracy: 0.9250\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 1 | global_acc: 35.135% | global_loss: 1.1999661922454834\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.4234 - categorical_accuracy: 0.9350\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 1 | global_acc: 35.135% | global_loss: 1.2000898122787476\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.4545 - categorical_accuracy: 0.9267\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 1 | global_acc: 35.135% | global_loss: 1.200042724609375\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.5616 - categorical_accuracy: 0.9033\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 1 | global_acc: 32.432% | global_loss: 1.1353118419647217\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.4898 - categorical_accuracy: 0.8967\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 1 | global_acc: 35.473% | global_loss: 1.1247364282608032\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.4342 - categorical_accuracy: 0.9133\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 1 | global_acc: 42.399% | global_loss: 1.0956203937530518\n",
            "6400\n",
            "19/19 [==============================] - 1s 52ms/step\n",
            "comm_round: 1 | global_acc: 35.135% | global_loss: 1.1228896379470825\n",
            "10/10 [==============================] - 2s 243ms/step - loss: 0.4014 - categorical_accuracy: 0.9417\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 2 | global_acc: 37.331% | global_loss: 1.164750337600708\n",
            "6400\n",
            "10/10 [==============================] - 2s 243ms/step - loss: 0.3439 - categorical_accuracy: 0.9667\n",
            "19/19 [==============================] - 1s 49ms/step\n",
            "comm_round: 2 | global_acc: 43.243% | global_loss: 1.0828008651733398\n",
            "6400\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.3812 - categorical_accuracy: 0.9600\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 2 | global_acc: 35.135% | global_loss: 1.1997398138046265\n",
            "6400\n",
            "10/10 [==============================] - 2s 246ms/step - loss: 0.3699 - categorical_accuracy: 0.9567\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 2 | global_acc: 43.750% | global_loss: 1.0809087753295898\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.3478 - categorical_accuracy: 0.9650\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 2 | global_acc: 35.135% | global_loss: 1.1999404430389404\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3680 - categorical_accuracy: 0.9717\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 2 | global_acc: 28.378% | global_loss: 1.1369727849960327\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.5567 - categorical_accuracy: 0.8700\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 2 | global_acc: 32.264% | global_loss: 1.1356098651885986\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3491 - categorical_accuracy: 0.9733\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 2 | global_acc: 35.135% | global_loss: 1.2000935077667236\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.3652 - categorical_accuracy: 0.9550\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 2 | global_acc: 35.135% | global_loss: 1.1980388164520264\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3876 - categorical_accuracy: 0.9400\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 2 | global_acc: 36.655% | global_loss: 1.1438602209091187\n",
            "6400\n",
            "19/19 [==============================] - 1s 53ms/step\n",
            "comm_round: 2 | global_acc: 35.642% | global_loss: 1.1266758441925049\n",
            "10/10 [==============================] - 2s 244ms/step - loss: 0.3625 - categorical_accuracy: 0.9683\n",
            "19/19 [==============================] - 1s 49ms/step\n",
            "comm_round: 3 | global_acc: 35.304% | global_loss: 1.185585618019104\n",
            "6400\n",
            "10/10 [==============================] - 2s 246ms/step - loss: 0.3542 - categorical_accuracy: 0.9717\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 3 | global_acc: 35.135% | global_loss: 1.126827597618103\n",
            "6400\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.3626 - categorical_accuracy: 0.9650\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 3 | global_acc: 35.135% | global_loss: 1.1924442052841187\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.3178 - categorical_accuracy: 0.9850\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 3 | global_acc: 37.500% | global_loss: 1.127916693687439\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.3220 - categorical_accuracy: 0.9650\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 3 | global_acc: 35.642% | global_loss: 1.1535996198654175\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.2977 - categorical_accuracy: 0.9883\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 3 | global_acc: 36.149% | global_loss: 1.146255612373352\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3150 - categorical_accuracy: 0.9817\n",
            "19/19 [==============================] - 1s 52ms/step\n",
            "comm_round: 3 | global_acc: 39.189% | global_loss: 1.0877193212509155\n",
            "6400\n",
            "10/10 [==============================] - 3s 254ms/step - loss: 0.4217 - categorical_accuracy: 0.9567\n",
            "19/19 [==============================] - 1s 57ms/step\n",
            "comm_round: 3 | global_acc: 35.135% | global_loss: 1.197972297668457\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.3887 - categorical_accuracy: 0.9550\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 3 | global_acc: 41.216% | global_loss: 1.1039990186691284\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3490 - categorical_accuracy: 0.9700\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 3 | global_acc: 35.135% | global_loss: 1.118183970451355\n",
            "6400\n",
            "19/19 [==============================] - 1s 52ms/step\n",
            "comm_round: 3 | global_acc: 36.318% | global_loss: 1.1265949010849\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.3639 - categorical_accuracy: 0.9667\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 4 | global_acc: 70.946% | global_loss: 0.9352229237556458\n",
            "6400\n",
            "10/10 [==============================] - 2s 246ms/step - loss: 0.3059 - categorical_accuracy: 0.9883\n",
            "19/19 [==============================] - 1s 49ms/step\n",
            "comm_round: 4 | global_acc: 56.081% | global_loss: 1.0068442821502686\n",
            "6400\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.3610 - categorical_accuracy: 0.9617\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 4 | global_acc: 34.966% | global_loss: 1.1236250400543213\n",
            "6400\n",
            "10/10 [==============================] - 2s 246ms/step - loss: 0.3738 - categorical_accuracy: 0.9550\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 4 | global_acc: 35.642% | global_loss: 1.1234891414642334\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.2860 - categorical_accuracy: 0.9900\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 4 | global_acc: 36.655% | global_loss: 1.1119048595428467\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3040 - categorical_accuracy: 0.9833\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 4 | global_acc: 41.892% | global_loss: 1.0827597379684448\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.3018 - categorical_accuracy: 0.9933\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 4 | global_acc: 51.689% | global_loss: 1.0261166095733643\n",
            "6400\n",
            "10/10 [==============================] - 3s 250ms/step - loss: 0.3097 - categorical_accuracy: 0.9883\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 4 | global_acc: 34.122% | global_loss: 1.1113498210906982\n",
            "6400\n",
            "10/10 [==============================] - 3s 250ms/step - loss: 0.3132 - categorical_accuracy: 0.9867\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 4 | global_acc: 42.230% | global_loss: 1.1015141010284424\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3057 - categorical_accuracy: 0.9850\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 4 | global_acc: 45.777% | global_loss: 1.0625331401824951\n",
            "6400\n",
            "19/19 [==============================] - 1s 52ms/step\n",
            "comm_round: 4 | global_acc: 53.209% | global_loss: 1.0571002960205078\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3345 - categorical_accuracy: 0.9833\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 5 | global_acc: 56.081% | global_loss: 0.9866078495979309\n",
            "6400\n",
            "10/10 [==============================] - 3s 252ms/step - loss: 0.3751 - categorical_accuracy: 0.9717\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 5 | global_acc: 59.797% | global_loss: 0.9709528088569641\n",
            "6400\n",
            "10/10 [==============================] - 2s 246ms/step - loss: 0.3087 - categorical_accuracy: 0.9850\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 5 | global_acc: 47.128% | global_loss: 1.0326342582702637\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.2924 - categorical_accuracy: 0.9967\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 5 | global_acc: 54.392% | global_loss: 1.0022928714752197\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.2647 - categorical_accuracy: 0.9950\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 5 | global_acc: 60.642% | global_loss: 0.9734306335449219\n",
            "6400\n",
            "10/10 [==============================] - 3s 265ms/step - loss: 0.3143 - categorical_accuracy: 0.9850\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 5 | global_acc: 58.784% | global_loss: 0.9819630980491638\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.3332 - categorical_accuracy: 0.9767\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 5 | global_acc: 43.412% | global_loss: 1.0605827569961548\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.3050 - categorical_accuracy: 0.9850\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 5 | global_acc: 48.649% | global_loss: 1.02174973487854\n",
            "6400\n",
            "10/10 [==============================] - 3s 250ms/step - loss: 0.3461 - categorical_accuracy: 0.9783\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 5 | global_acc: 62.669% | global_loss: 1.0572307109832764\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3593 - categorical_accuracy: 0.9700\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 5 | global_acc: 37.838% | global_loss: 1.118181586265564\n",
            "6400\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 5 | global_acc: 56.250% | global_loss: 0.9895864129066467\n",
            "10/10 [==============================] - 2s 244ms/step - loss: 0.3179 - categorical_accuracy: 0.9883\n",
            "19/19 [==============================] - 1s 49ms/step\n",
            "comm_round: 6 | global_acc: 54.392% | global_loss: 1.0307002067565918\n",
            "6400\n",
            "10/10 [==============================] - 3s 253ms/step - loss: 0.3326 - categorical_accuracy: 0.9867\n",
            "19/19 [==============================] - 1s 49ms/step\n",
            "comm_round: 6 | global_acc: 46.453% | global_loss: 1.0399285554885864\n",
            "6400\n",
            "10/10 [==============================] - 2s 246ms/step - loss: 0.3392 - categorical_accuracy: 0.9617\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 6 | global_acc: 53.209% | global_loss: 1.0008893013000488\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.2875 - categorical_accuracy: 0.9933\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 6 | global_acc: 76.520% | global_loss: 0.8520181179046631\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.2558 - categorical_accuracy: 1.0000\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 6 | global_acc: 76.351% | global_loss: 0.8920669555664062\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.2700 - categorical_accuracy: 0.9950\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 6 | global_acc: 66.892% | global_loss: 0.9272527694702148\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.2994 - categorical_accuracy: 0.9933\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 6 | global_acc: 57.432% | global_loss: 0.9728624224662781\n",
            "6400\n",
            "10/10 [==============================] - 3s 253ms/step - loss: 0.3276 - categorical_accuracy: 0.9783\n",
            "19/19 [==============================] - 1s 53ms/step\n",
            "comm_round: 6 | global_acc: 52.365% | global_loss: 1.027415156364441\n",
            "6400\n",
            "10/10 [==============================] - 3s 254ms/step - loss: 0.3337 - categorical_accuracy: 0.9733\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 6 | global_acc: 73.311% | global_loss: 0.9077171087265015\n",
            "6400\n",
            "10/10 [==============================] - 2s 249ms/step - loss: 0.2932 - categorical_accuracy: 0.9917\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 6 | global_acc: 60.980% | global_loss: 0.9729164838790894\n",
            "6400\n",
            "19/19 [==============================] - 1s 52ms/step\n",
            "comm_round: 6 | global_acc: 58.615% | global_loss: 0.9671174883842468\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.3921 - categorical_accuracy: 0.9450\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 7 | global_acc: 79.392% | global_loss: 0.8557619452476501\n",
            "6400\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.3175 - categorical_accuracy: 0.9900\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 7 | global_acc: 76.689% | global_loss: 0.8708370327949524\n",
            "6400\n",
            "10/10 [==============================] - 2s 245ms/step - loss: 0.3080 - categorical_accuracy: 0.9767\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 7 | global_acc: 80.236% | global_loss: 0.8567691445350647\n",
            "6400\n",
            "10/10 [==============================] - 2s 247ms/step - loss: 0.3298 - categorical_accuracy: 0.9817\n",
            "19/19 [==============================] - 1s 50ms/step\n",
            "comm_round: 7 | global_acc: 71.453% | global_loss: 0.9142495393753052\n",
            "6400\n",
            "10/10 [==============================] - 2s 248ms/step - loss: 0.3998 - categorical_accuracy: 0.9433\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 7 | global_acc: 50.845% | global_loss: 1.010947346687317\n",
            "6400\n",
            "10/10 [==============================] - 3s 250ms/step - loss: 0.3128 - categorical_accuracy: 0.9750\n",
            "19/19 [==============================] - 1s 51ms/step\n",
            "comm_round: 7 | global_acc: 59.122% | global_loss: 0.966729462146759\n",
            "6400\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.4001 - categorical_accuracy: 0.9453"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the confusion matrix. Set Normalize = True/False\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        cm = np.around(cm, decimals=2)\n",
        "        cm[np.isnan(cm)] = 0.0\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    thresh = cm.max() / 2.\n",
        "\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predict_x=global_model.predict(X_test) \n",
        "y_pred=np.argmax(predict_x,axis=1)\n",
        "\n",
        "y_testreport=np.argmax(y_test,axis=1)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Print Classification Report\n",
        "\n",
        "print('Classification Report')\n",
        "print(classification_report(y_testreport, y_pred, target_names=[\"lung_aca\", \"lung_n\", \"lung_scc\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNdvli1fnhQ7",
        "outputId": "0c8a5404-c51c-40dc-bcff-f51e1e064998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 1s 55ms/step\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    lung_aca       0.87      1.00      0.93       192\n",
            "      lung_n       1.00      1.00      1.00       208\n",
            "    lung_scc       1.00      0.85      0.92       192\n",
            "\n",
            "    accuracy                           0.95       592\n",
            "   macro avg       0.96      0.95      0.95       592\n",
            "weighted avg       0.96      0.95      0.95       592\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "print('Confusion Matrix')\n",
        "cm = confusion_matrix(y_testreport, y_pred)\n",
        "plot_confusion_matrix(cm, [\"yes\",\"no\"], title='Confusion Matrix')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "-We7MlN7oRMC",
        "outputId": "f3af3cd0-a15d-4e77-8717-832eaf444c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "Confusion matrix, without normalization\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAFgCAYAAACSQzOFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5xU1f3/8dcbEURBBUHExQKC2CUKauwlxl5jjbEnaoIxRRNLjEaN3xg1MSYm5oexxK4EGxbUGE3UaKSIDRuKRooFbFgisnx+f9y7OKywO+zM7Mwc3s885sHec+/c+5l5xM+e/dxzz1FEYGZmtatDtQMwM7OWOVGbmdU4J2ozsxrnRG1mVuOcqM3MalzHagdgZlYpSyy7WsScT0s6R3z6zr0RsXOZQmoTJ2ozS1bM+ZTOgw4o6Rz/m/DHnmUKp82cqM0sYQLVf4XXidrM0iVAqnYUJav/XzVmZolzj9rM0ubSh5lZjUug9OFEbWYJS+NmYv1/AjOzKpK0iqQHJU2U9JykH+TtPSTdL+nl/N/uebsk/V7SJElPS9qotWs4UZtZ2qTSXq2bA5wYEesAmwHDJK0DnAI8EBEDgQfybYBdgIH56xjg0tYu4ERtZukSWemjlFcrImJ6RIzPf54FPA80AHsBf80P+yuwd/7zXsDVkXkcWF5Sn5au4Rq1mSWs6F5xea4mrQ58BfgP0Dsipue73gR65z83AG8UvG1K3jadhXCiNjNrWU9JYwu2h0fE8OYHSeoKjAR+GBEfquAXRESEpDYvp+VEbWZpK33Ux4yIGNLiJaQlyZL0dRFxS978lqQ+ETE9L228nbdPBVYpeHvfvG2hXKM2s7RV+Gaisq7z5cDzEfHbgl13AIfnPx8O3F7Qflg++mMz4IOCEskCuUdtZglrl3HUWwCHAs9ImpC3nQacB9ws6WjgdaBpGr+7gV2BScAnwJGtXcCJ2szS1Q6TMkXEI/mVFmSHBRwfwLBFuYZLH2ZmNc49ajNLWwKPkDtRm1nC0pjrw4nazNLWof5nz6v/XzVmZolzj9rM0tU010edc6I2s7R54QAzs1qWxs3E+v8EZmaJc6K2kkjqImmUpA8kjSjhPIdIuq+csVWDpHskHd76kdZuKr9wQMU5US8mJH1T0lhJH0manieULctw6v3I5tldISL2b+tJIuK6iPh6GeKZj6RtJYWkW5u1b5i3P1TkeX4h6drWjouIXSLir60dZ+2owgsHtIfaiMIqStKPgd8B/0eWVFcF/kS20kSpVgNeiog5ZThXpbwDfFXSCgVthwMvlesC+Uxo/u+p1pTam3aP2tqDpOWAs4FhEXFLRHwcEZ9HxKiI+El+TGdJv5M0LX/9TlLnfN+2kqZIOlHS23lv/Mh831nAGcCBeU/96OY9T0mr5z3Xjvn2EZJelTRL0mRJhxS0P1Lwvs0ljclLKmMkbV6w7yFJ50h6ND/PfZJ6tvA1zAZuAw7K378EcCBwXbPv6mJJb0j6UNI4SVvl7TuTzYbW9DmfKojjXEmPks2C1j9v+3a+/1JJIwvO/2tJD0g18l//4sI9aqsDXwWWAm5t4ZifkS3KORjYENgEOL1g/0rAcmTLBR0N/FFS94g4k6yXflNEdI2Iy1sKRNIywO+BXSKiG7A5MGEBx/UA7sqPXQH4LXBXsx7xN8mmh1wR6ASc1NK1gauBw/KfdwKeBaY1O2YM2XfQA7geGCFpqYgY3exzbljwnkPJFijtRjaVZaETgfXzX0JbkX13h+ezp5kVzYk6fSuQrVDRUmniEODsiHg7It4BziJLQE0+z/d/HhF3Ax8Bg9oYz1xgPUld8kVBn1vAMbsBL0fENRExJyJuAF4A9ig45sqIeCkiPgVuJkuwCxUR/wZ6SBpElrCvXsAx10bEzPyavwE60/rnvCoinsvf83mz831C9j3+FrgW+H5ETGnlfFZuLn1YHZhJtuZbS2PmV2b+3uDredu8czRL9J8AXRc1kIj4mKzkcBwwXdJdktYqIp6mmBoKtt9sQzzXAMcD27GAvzAknSTp+bzc8j7ZXxEtlVRg/kVKvyQi/gO8SvaM3M1FxGhlJZc+rC48BnzGF0vVL8g0spuCTVbly2WBYn0MLF2wvVLhzoi4NyJ2BPqQ9ZIvKyKepphaXFeuCNcA3wPuznu78+SliZ+SrcLRPSKWBz6AeRPCL6xc0WIZQ9Iwsp75tPz81t7co7ZaFxEfkN3w+6OkvSUtLWlJSbtIOj8/7AbgdEm98ptyZ5D9qd4WE4CtJa2a38g8tWmHpN6S9spr1Z+RlVDmLuAcdwNr5kMKO0o6EFgHuLONMQEQEZOBbchq8s11A+aQjRDpKOkMYNmC/W8Bqy/KyA5JawK/BL5FVgL5qaQWSzRmC+JEvRjI660/JrtB+A7Zn+vHk42EgCyZjAWeBp4BxudtbbnW/cBN+bnGMX9y7ZDHMQ14lyxpfncB55gJ7E52M24mWU9094iY0ZaYmp37kYhY0F8L9wKjyYbsvQ78j/nLGk0P88yUNL616+SlpmuBX0fEUxHxMtnIkWuaRtRYO2ialKnOSx/yDWgzS1WH5VeLzludXNI5/nfnsHERMaRMIbWJJ2Uys7TVSJ25FLXRrzczs4Vyj9rM0lYjdeZSOFGbWdoSKH3UZaJWp2VCS3WvdhjJ+MrAlVo/yKxKxo8fNyMierXpzUpj4YD6TNRLdafz0GHVDiMZj957ausHmVVJlyXV/CnVRZNAj7r+f9WYmSWuLnvUZmbFSmFWWSdqM0uWcKI2M6tt4otpteqYa9RmZjXOPWozS5hc+jAzq3WVTtSSriCb7fHtiFgvb7uJL1YHWh54PyIGS1odeB54Md/3eEQc19o1nKjNLGnt0KO+CriEguXdIuLAguv/hmwRiiavRMQizUvuRG1mVoKI+FfeU/6SfMX5A4DtS7mGbyaaWdIklfQiW3N0bMHrmEW4/FbAW/nCEU36SXpS0j/zJeBa5R61maWrPMPzZpSwcMDBZEvdNZkOrBoRMyVtDNwmad2I+LClkzhRm1myVMVRH/lybPsCGze1RcRnZOuFEhHjJL0CrEm2FN5COVGbWdKqODzva8ALETGlIJZewLsR0SipPzAQeLW1E7lGbWZWAkk3AI8BgyRNkXR0vusg5i97AGwNPC1pAvA34LiIeLe1a7hHbWZJq3SPOiIOXkj7EQtoGwmMXNRrOFGbWdL8ZKKZWS3zpExmZtYe3KM2s6S59GFmVsOqOY66nJyozSxpKSRq16jNzGqce9Rmlrb671A7UZtZwpRG6cOJ2syS5kRtZlbjUkjUvploZlbj3KM2s2R5HLWZWT2o/zztRG1mCUtk1Idr1GZmNc49ajNLWgo9aidqM0taConapY8S/PmkXXn9bycw9i/fnte2fv8VeegPhzHmsqP52y/3o9vSnQDYfuPVefTSIxhz2dE8eukRbDN4tWqFXZfuu3c0G6w7iHXXGsAF559X7XDq2mL3XarEVw1woi7BNfc+w16n3jRf26Un7srplz3I0O9czh2PvMSPDtgMgJkffMp+p/+Nod+5nO/8+k6uOHWPaoRclxobG/nhCcO4fdQ9PPn0REbceAPPT5xY7bDqkr/L+uREXYJHn3mDdz/833xtA/p255Gn3wDgH+Mms/fWgwB4atJbTJ/5EQATX5vBUp060mnJJdo34Do15oknWGONAfTr359OnTqx/4EHceeo26sdVl1aHL9LSSW9aoETdZk9//oM9thiIAD7brMWfXt1+9Ix+2w9iAkvv8nszxvbO7y6NG3aVPr2XWXedkNDX6ZOnVrFiOrX4vZdlpqknagTdewFd3HMnhvz6KVH0LVLZ2bPmTvf/rVX68kvv7Mdx180ukoRmi1eUkjUHvVRZi+98S57nHwjAAP69mCXzdaYt6+hZzduOvsbfPu8UUye/n61Qqw7K6/cwJQpb8zbnjp1Cg0NDVWMqH4tjt9lrSTbUlSkRy3pbEk/LNg+V9IPJP1E0hhJT0s6K9+3jKS7JD0l6VlJB1YipvbSa/mlAZDglEM257JRTwKw3DKdueX/9ufnlz3IY8+l+6dmJQwZOpRJk17mtcmTmT17NiNuupHddt+z2mHVJX+X9alSPeorgFuA30nqABwEnAbsAGxCNujlDklbA72AaRGxG4Ck5RZ0QknHAMcA0Hn5CoW9aP76s73YasNV6blcFybdOIxz/vowXbt04ti9Ngbg9odf5OrRTwNw3N4bs8bK3Tn10C059dAtAdjj5Bt55/1PqhZ/vejYsSMXXXwJe+y2E42NjRx+xFGss+661Q6rLi2W32X9d6hRRFTmxNL9wE+B3sC3gdeA/YCmv/m7Ar8CHgbuA24C7oyIh1s7d4dl+0bnocMqEPXi6b17T612CGYL1WVJjYuIIW15b+feA6PhkItLuv7ki3Zr8/XLpZI16r8ARwArkfWwdwB+FRH/r/mBkjYCdgV+KemBiDi7gnGZ2eIikUmZKpmobwXOBpYEvgnMAc6RdF1EfCSpAfg8j+HdiLhW0vtkvW8zM8tVLFFHxGxJDwLvR0QjcJ+ktYHH8t9wHwHfAgYAF0iaS5a4v1upmMxs8SKyG/v1rmKJOr+JuBmwf1NbRFwMNC8YvQLcW6k4zGxxVjtjoUtRqeF56wCTgAci4uVKXMPMrBhSaa/Wz68rJL0t6dmCtl9ImippQv7atWDfqZImSXpR0k7FfIaK9KgjYiLQvxLnNjNbFO3Qo74KuAS4uln7RRFxYbNY1iEbrrwusDLwd0lr5uXhhfIj5GZmJYiIfwHvFnn4XsCNEfFZREwmqzxs0tqbnKjNLF0llj3yznhPSWMLXscUefXj86ewr5DUPW9rAN4oOGZK3tYiz/VhZskS0KFDyaWPGW144OVS4Bwg8n9/AxzV1gCcqM0sadUY9BERb31xfV0G3JlvTgVWKTi0b97WIpc+zMzKTFKfgs19gKYRIXcAB0nqLKkfMBB4orXzuUdtZkmr9KgPSTcA25LVsqcAZwLbShpMVvp4DTgWICKek3QzMJHsae1hrY34ACdqM0tZkWOhSxERBy+g+fIWjj8XOHdRruFEbWbJyh4h95OJZmZWYe5Rm1nC0pjrw4nazJKWQJ52ojaztLlHbWZWy9ph1Ed78M1EM7Ma5x61mSUrleF5TtRmlrQE8rQTtZmlLYUetWvUZmY1zj1qM0taAh1qJ2ozS5jSKH04UZtZsrJRH9WOonSuUZuZ1Tj3qM0sYZ6Uycys5iWQp52ozSxt7lGbmdUyT8pkZmbtwT1qM0uWJ2UyM6sDTtRmZjUugTztGrWZWa1zj9rMkubSh5lZLUtkeF5dJurBA1fi0dGnVDuMZHQfeny1Q0jKe2MuqXYIllMij5C7Rm1mVuPqskdtZlasBDrUTtRmlrYOCWRqJ2ozS1oCedo1ajNLl/KluEp5tX4NXSHpbUnPFrRdIOkFSU9LulXS8nn76pI+lTQhf/25mM/hRG1mVpqrgJ2btd0PrBcRGwAvAacW7HslIgbnr+OKuYATtZklrYNKe7UmIv4FvNus7b6ImJNvPg70LekzlPJmM7NaV4bSR09JYwtexyxiCEcB9xRs95P0pKR/StqqmBP4ZqKZJa0MNxNnRMSQtl1bPwPmANflTdOBVSNipqSNgdskrRsRH7Z0HveozcwqQNIRwO7AIRERABHxWUTMzH8eB7wCrNnaudyjNrNkiewx8na/rrQz8FNgm4j4pKC9F/BuRDRK6g8MBF5t7XxO1GaWtGJuCJZC0g3AtmS17CnAmWSjPDoD9+d17sfzER5bA2dL+hyYCxwXEe8u8MQFnKjNLF1FjoUuRUQcvIDmyxdy7Ehg5KJewzVqM7Ma5x61mSUthUfInajNLFnCkzKZmdW8BPK0E7WZpc0rvJiZWcW5R21myVLqi9tK+gMQC9sfESdUJCIzszJK/Wbi2HaLwsysQuo/TbeQqCPir4XbkpYufGbdzMzaR6s3EyV9VdJE4IV8e0NJf6p4ZGZmZVDppbjaQzGjPn4H7AQ0Tc33FNnEImZmNS174KWyK7y0h6JGfUTEG81+szRWJhwzszKqoV5xKYpJ1G9I2hwISUsCPwCer2xYZmbWpJhEfRxwMdAATAPuBYZVMigzs3JJoEPdeqKOiBnAIe0Qi5lZ2aVQ+ihm1Ed/SaMkvSPpbUm350vImJnVtFRuJhYz6uN64GagD7AyMAK4oZJBmZmVy+IyPG/piLgmIubkr2uBpSodmJmZZVqa66NH/uM9kk4BbiSb++NA4O52iM3MrGS10ScuTUs3E8eRJeamz3lswb4gW2XXzKxmSYlPyhQR/dozkJQc+52jGH33XfTqtSJjJzxT7XDqQt/ey/OXcw5jxRW6EQFXjHyUP97wEN2XXZprfn0Uq63cg9envcu3fno578/6lB8dtgMH7joUgI5LdGCtfiuxyvan8N6Hno6mNffdO5qTfvwDGhsbOeKob/OTn55S7ZAqKoE8XdzCAZLWk3SApMOaXpUOrJ4detgR3HbnPdUOo67MaZzLKb+9hY2+cS7bHHYhxx64NWv1X4mTjtyRh554kfX3OpuHnniRk478OgAXXf0Amx10HpsddB5n/OEOHh73spN0ERobG/nhCcO4fdQ9PPn0REbceAPPT5xY7bCsFcUMzzsT+EP+2g44H9izwnHVtS232poe3Xu0fqDN8+aMD5nwwhQAPvrkM16Y/CYr91qe3bfdgGtH/QeAa0f9hz222+BL7z1g5yHcPHpcu8Zbr8Y88QRrrDGAfv3706lTJ/Y/8CDuHHV7tcOqqMVl1Md+wA7AmxFxJLAhsFxFo7LF2qp9ejB4UF/GPPsaK67QjTdnfAhkyXzFFbrNd2yXpZZkx83X5rYHJlQj1LozbdpU+vZdZd52Q0Nfpk6dWsWIKq9plZe2vmpBMY+QfxoRcyXNkbQs8DawSmtvMmuLZbp04oYLv81PLhzJrI//96X90WzNod22Xp/HJrzqsoctkFASNxOL6VGPlbQ8cBnZSJDxwGMVjcoWSx07duCGC7/DTfeM5fZ/PAXA2zNnsVLPZQFYqeeyvPPurPnes/9OGzPCZY+irbxyA1OmvDFve+rUKTQ0NFQxIitGq4k6Ir4XEe9HxJ+BHYHD8xLIIpO0uqTnJV0m6TlJ90nqImmwpMclPS3pVknd23J+q29/PvMQXpz8Jr+/9h/z2u765zN8a49NAfjWHpty50NPz9u3bNel2HLjAYwqaLOWDRk6lEmTXua1yZOZPXs2I266kd12T/iWU4llj1rpjC80UUvaqPkL6AF0zH9uq4HAHyNiXeB94BvA1cDJEbEB8AxwZgnnr7rDv/VNtt16c1566UUG9FuFq668vNoh1bzNB/fnkN03ZZuha/L4jafw+I2nsNOW63Dhlfez/aZr8cztZ7DdpoO48Mr7571nz+025IHHX+CT/82uYuT1pWPHjlx08SXssdtODF5/bb6x/wGss+661Q6rolK4mahoXvRr2iE92ML7IiK2X+SLSasD90fEwHz7ZLLH0Y+OiFXztjWAERGxUbP3HgMcA7DKqqtu/OKk1xb18rYQPTb5frVDSMp7Yy6pdghJ6bKkxkXEkLa8d8UB68WBF4wo6fqX7LtOm69fLi098LJdha75WcHPjcDyxbwpIoYDwwE22njIgn+7mJkVEIvJNKft4APgPUlb5duHAv+sYjxmZkWTdEU+BfSzBW09JN0v6eX83+55uyT9XtKk/J5cUWXkWkjUAIcDF0h6GhgMnF3leMwsEe0wH/VVwM7N2k4BHsjLvA/k2wC7kN2nG0hWyr20mAsUtbhtuUTEa8B6BdsXFuzerD1jMbPFQ6Un/4+If+X33wrtBWyb//xX4CHg5Lz96shuDj4uaXlJfSJiekvXKOYRckn6lqQz8u1VJW2yKB/EzKwasiF2JY/66ClpbMHrmCIu3bsg+b4J9M5/bgDeKDhuSt7WomJ61H8C5gLbk5UkZgEjgaFFvNfMrN7NKGXUR0SEpJIGQBSTqDeNiI0kPZlf9D1JnUq5qJlZe6nSuodvNZU0JPUhm3oDYCrzT8HRN29rUTE3Ez+XtATZYgFI6kXWwzYzq3lVejLxDrJBEuT/3l7QflheUt4M+KC1+jQU16P+PXArsKKkc8lm0zt9kcM2M2tn2Srkle1SS7qB7MZhT0lTyJ6sPg+4WdLRwOvAAfnhdwO7ApOAT4CipuNoNVFHxHWSxpFNdSpg74h4ftE+iplZdVR6DHJEHLyQXTss4NgAhi3qNVpN1JJWJcv8owrbIuK/i3oxMzNbdMWUPu7ii0VulwL6AS8Cac/kYmZJSOAJ8qJKH+sXbuePPH6vYhGZmZWJlMbCAYv8ZGJEjJe0aSWCMTMrtwTydFE16h8XbHYANgKmVSwiMzObTzE96sLVROeQ1axHViYcM7PyqtIDL2XVYqLOH3TpFhEntVM8ZmZl0x7jqNvDQhO1pI4RMUfSFu0ZkJlZOSWQp1vsUT9BVo+eIOkOYATwcdPOiLilwrGZmRnF1aiXAmaSzZ7XNJ46ACdqM6ttxU/+X9NaStQr5iM+nuWLBN3EaxaaWV0Q9Z+pW0rUSwBdYYGf0onazGpedjOx2lGUrqVEPT0ivHahmdW1FBJ1SxNLJfDxzMzqX0s96i9N0WdmVm+UwPi8hSbqiHi3PQMxMyu3xaFGbWZW30pbTqtmVHrxAzMzK5F71GaWtKTn+jAzq3euUZuZ1YEEOtSuUZuZ1Tr3qM0sYaJDAs/uOVGbWbJEGqUPJ2ozS9diMM2pmVndS2F4nm8mmpnVOPeozSxZrlGbmdWBFEofTtRmlrQE8nR9JurPG+cy7b3/VTuMZLw35pJqh5CUfsNGVjsES0xdJmozs2KIyo+YkDQIuKmgqT9wBrA88B3gnbz9tIi4uy3XcKI2s3Sp8iu8RMSLwGAASUsAU4FbgSOBiyLiwlKv4URtZklr5xL1DsArEfF6OX9BeBy1mVnLekoaW/A6poVjDwJuKNg+XtLTkq6Q1L2tAThRm1mysvmoVdILmBERQwpewxd4LakTsCcwIm+6FFiDrCwyHfhNWz+HSx9mlrR2LH3sAoyPiLcAmv4FkHQZcGdbT+xEbWZJa8dx1AdTUPaQ1Ccipueb+wDPtvXETtRmljBVfNQHgKRlgB2BYwuaz5c0GAjgtWb7FokTtZlZiSLiY2CFZm2Hluv8TtRmlqz2eOClPThRm1nS2qP0UWlO1GaWtPpP02n8VWBmljT3qM0sXe0w10d7cKI2s2T5ZqKZWR1IoUedwi8bM7OkuUdtZkmr//60E7WZJS6ByocTtZmlK7uZWP+Z2onazJKWQo/aNxPNzGqce9RmljAhlz7MzGpbCqUPJ2ozS1YqNxNdozYzq3HuUZtZuuTSh5lZzXOiNjOrcSmM+nCN2sysxrlHbWbJEtCh/jvUTtRmlrYUSh9O1GaWNN9MNDOrcSn0qH0zsUymTZ3CIfvszE5bbcTOW2/MVcP/CMDEZ5/iG7tswx7bb8reX9+Cp8aPqXKk9em+e0ezwbqDWHetAVxw/nnVDqcu/PawjXnmgt148Iyvzdd+1HZr8PBZX+ehM3fk9H3Xm29fQ/cuTLp4L47bcWB7hmqtcI+6TDp2XIJTz/oV623wFT76aBZ777gFW2yzPb8++3ROOOk0ttlhJx76+2h+fc7pXH/rvdUOt640NjbywxOGcdc999PQty9bbjaU3Xffk7XXWafaodW0mx97nSsffIXfHzlkXtvma/Zipw1XZodz/s7sOXNZoVvn+d7zi/034B/PvdneoVaMbybafFbs3YcVe/cBoGvXbqwxcBBvvTkNSXw0axYAsz78kN75MVa8MU88wRprDKBf//4A7H/gQdw56nYn6lY8/vIM+q6w9Hxth2/Tn0tGv8jsOXMBmDnrs3n7dt5wZf478xM++WxOu8ZZWZ49zxZiyn9fZ+KzT7HhRkM5/ZzzOfKgPfnVWacSc+dy850PVju8ujNt2lT69l1l3nZDQ1+eeOI/VYyofvXv3ZVNB67AKXuvy2efN3LW357hqdffY+nOSzBs5zU58HcP890d16x2mOWTyCPkrlGX2ccff8Swow/m9HPOp1u3Zbn+qsv42dnn88iTL3Pa2edz6o++W+0QbTHWsYNYfplO7Hbeg5w98hmGH7MpACftvg7D//4yn3zWWOUIbUHcoy6jzz//nGFHfZM9v3EQO+22NwC33HwdPz/3QgB23XNfTvvx96oZYl1aeeUGpkx5Y9721KlTaGhoqGJE9Wv6+59y9/hpAEx47T3mRrBC105s1K8Hu2/UwM/3XZ9ll16SuQGffT6XKx96pcoRly6BDrUTdblEBKf+6LsMGDiIo487YV5775X68J9/P8xmW2zNYw8/xOr916hilPVpyNChTJr0Mq9NnszKDQ2MuOlGrrrm+mqHVZdGT5jGFoN68e+X3qH/il1ZcokOzPxoNntf+M95x5y4+9p8/NmcZJJ0h3aofUh6DZgFNAJzImKIpB7ATcDqwGvAARHxXlvO70RdJuOeeIzbRlzPoLXXY4/tsz8nTzztLM79zR855/STaJzTSOfOnTn3wkuqHGn96dixIxddfAl77LYTjY2NHH7EUayz7rrVDqvm/enoTdh8UE96dO3MuPN24cJRz3PDo69x0eFDePCMr/F541x+cNXYaodZce3Yo94uImYUbJ8CPBAR50k6Jd8+uS0nVkSUI8B2tf7gjeK2+x6tdhjJaOjRpdohJKXfsJHVDiEpbw7fb1xEDGn9yC9be/2vxJW3lXYD/6sDurd6/bxHPaQwUUt6Edg2IqZL6gM8FBGD2hKDbyaamZUugPskjZN0TN7WOyKm5z+/CfRu68ld+jCzpJVhHHVPSYU1ouERMbzZMVtGxFRJKwL3S3qhcGdEhKQ2ly+cqM0saWW4lzijtdJHREzN/31b0q3AJsBbkvoUlD7ebmsALn2YWdJU4qvV80vLSOrW9DPwdeBZ4A7g8Pyww4Hb2/oZ3KM2MytNb+BWZV33jsD1ETFa0hjgZklHA68DB7T1Ak7UZpa2Co/Pi4hXgQ0X0D4T2KEc13CiNrNkZeWL+n820YnazNLlSZnMzKw9uEdtZklLoEPtRG1miT9rjj8AAAkFSURBVEsgUztRm1nCvMKLmVnN881EMzOrOPeozSxZxT4GXuucqM0sbQlkaidqM0taCjcTXaM2M6tx7lGbWdJSGPXhRG1mSUsgTztRm1nCEhn24Rq1mVmNc4/azJKWwqgPJ2ozS5bwzUQzs5qXQJ52ojazxCWQqX0z0cysxrlHbWZJ881EM7Ma55uJZmY1LoE87Rq1mVmtc4/azNKWQJfaidrMkpVN9VH/mdqJ2szSpTRuJrpGbWZW49yjNrOkJdChdqI2s8QlkKld+jCzhKnk/7V6BWkVSQ9KmijpOUk/yNt/IWmqpAn5a9e2fgr3qM0sae1wM3EOcGJEjJfUDRgn6f5830URcWGpF3CiNjMrQURMB6bnP8+S9DzQUM5r1GWifvapJ2cM6L3069WOowg9gRnVDiIh/j4XT6u19Y3tvWSipNWBrwD/AbYAjpd0GDCWrNf9XlvOW5eJOiJ6VTuGYkgaGxFDqh1HKvx9WpuUnql7ShpbsD08IoZ/6TJSV2Ak8MOI+FDSpcA5QOT//gY4qi0B1GWiNjMrVhmeTJzRWgdB0pJkSfq6iLgFICLeKth/GXBnWwPwqA8zsxJIEnA58HxE/LagvU/BYfsAz7b1Gu5RV9aX/jyykvj7tEXWDqM+tgAOBZ6RNCFvOw04WNJgstLHa8Cxbb2AE3UFLaiOZW3n79PaotJ5OiIeWchl7i7XNZyozSxdnpTJzMzag3vUZpa4+u9SO1GbWbKESx9WJEl9JC1d7ThSI8n//7VWqcRXLXCPusIk7Un2NNJPgJerHE5dk3QI0A+YBdwaEf+V1CEi5lY5NKth7lFbiyRtBZwFnBERL0taStJy+b4E/u/TfiQNA75PlqRXA0ZKGuAkbYsD96grQJIiIoC1gX8CjZK+B3wdmC3pJxFRD5NKVV3Bd7k+cEJEPJG3nwz8XNJxEfFpVYO0mpbC4rbuUVdGt/zfMUAXYATZ00mXAa8Ay1cprno0MJ9HoS+wbUH7PcBsJ2lrVQJFaveoy0zSbmSPjr4KjANOATpExExJXwH2Bm6qZoz1QtLxwA+BW4GngBMkzYiIK8h62GtIWi4iPqhmnFbbaiTXlsSJuowkDQUuAPYim6RldWB0tktbAlcCP4qICQs9iQHzbsJuAOxEVjJaFvg78Mv8F952wIFO0rY4cKIuE0mrAl8Fziab4H4p4OcR8ZmkBrKbYAdHxNgWTmNA/n1dAvw9Il6RdAXwjXz3m2STM/0iImZWK0arD/Ij5NZEUm+yEQkzgGPIEsk+EfG6pP2A75FNgegkXYSImEpW8thZ0kER8RlwI/AOMBd410nailXpxW3bg3vU5TEDGEg2xvdF4D5gWUkrAz8HTo+I2VWMr+5ExC2SPgN+JYmIuFHSVcAyETGryuFZPamNXFsSJ+oS5H+id42IFyWdAJwEvASsAFwKfEQ2hnpUwTAzK1JE3CVpLjBc0pyI+BtZCclsseJE3UaSliFLzBtKuhF4DFgSGB8R/5b0W2DJiHjXSbrtIuIeSUeRDWs0W2QJdKidqNsqIj6WdCqwDnAy0AvYDxgiad+IeKPgWCfpEkTE/dWOwepXCjcTnahLEBH/A8ZLOgboTHZzdjDZwxlvuCdtVm21c0OwFB71UQYR8UFEvB0R5wDjgSPzdidpsypqmua0lFctcKIuk4JJll4BVpPUpZrxmFk6nKjLJCIiT9YfAyd6DgozKxfXqMsoL3WMqnYcZvaFWilflMKJ2syS5puJZmZWce5Rm1m6amjkRinco7aFktQoaYKkZyWNKGWBXklX5RNUIekvktZp4dhtJW3ehmu8Jqlnse3NjvloEa/1C0knLWqM1r5KXTOgVnK8E7W15NOIGBwR6wGzgeMKd0pq019kEfHtiJjYwiHbAoucqM0WKIFM7URtxXoYGJD3dh+WdAcwUdISki6QNEbS05KOhWxcuaRLJL0o6e/Aik0nkvSQpCH5zztLGi/pKUkPSFqd7BfCj/Le/FaSekkamV9jjKQt8veuIOk+Sc9J+gtF/Gcl6TZJ4/L3HNNs30V5+wOSeuVta0ganb/nYUlrlePLNFsUrlFbq/Ke8y5kq9UAbASsFxGT82T3QUQMldQZeFTSfcBXgEFkc6H0BiYCVzQ7by+ydSS3zs/VI5/E6s/ARxFxYX7c9cBFEfFIvkDDvWQLB58JPBIRZ+dLoB1dxMc5Kr9GF2CMpJH53NbLAGMj4keSzsjPfTzZ3OLH5avIbwr8Cdi+DV+jVUkKoz6cqK0lXSQ1LRv2MNnyYpsDT0TE5Lz968AGTfVnYDmyubm3Bm6IiEZgmqR/LOD8mwH/ajpXRLy7kDi+BqzzxcOfLCupa36NffP33iXpvSI+0wmS9sl/XiWPdSbZggRNa1leC9ySX2NzYETBtTsXcQ2rISncTHSitpZ8GhGDCxvyhPVxYRPw/Yi4t9lxu5Yxjg7AZvkkWM1jKZqkbcmS/lcj4hNJD5EtmbYgkV/3/ebfgdWXBPK0a9RWsnuB70paEkDSmvlc3f8CDsxr2H3IFqNt7nFga0n98vf2yNtnAd0KjruPbKkz8uOaEue/gG/mbbsA3VuJdTngvTxJr0XWo2/SgWyaWvJzPhIRHwKTJe2fX0OSNmzlGlZrfDPRjL+Q1Z/HS3oW+H9kf6ndCryc77uabGGF+UTEO2RrTN4i6Sm+KD2MAvZpupkInEA2z/fTkibyxeiTs8gS/XNkJZD/thLraKCjpOeB88h+UTT5GNgk/wzbky1SDHAIcHQe33NkK8ybzSe/Kf6ipEmSTin7+T0Tp5mlaqONh8Sjj5e2pvTSnTQuIoYsbL+kJciW4NsRmAKMAQ5uZQjqInGP2syS1U7zUW8CTIqIV/NFrG+kzH95+WaimSVr/Phx93ZZsuWnUouwlKTCbvnwiBhesN0AvFGwPQXYtMRrzseJ2sySFRE7VzuGcnDpw8ysNFPJxuQ36Zu3lY0TtZlZacYAAyX1k9QJOAi4o5wXcOnDzKwEETFH0vFkzxQsAVwREc+V8xoenmdmVuNc+jAzq3FO1GZmNc6J2sysxjlRm5nVOCdqM7Ma50RtZlbjnKjNzGrc/wfd716juV0RyQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}